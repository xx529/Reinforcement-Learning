{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "欢迎使用 Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xx529/Reinforcement-Learning/blob/main/Policy%20Gradient/Reducing_Variance_Using_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaS_fAlOzQQQ"
      },
      "source": [
        "# Reducing_Variance_Using_Baseline\n",
        "\n",
        "* $\\nabla E_{\\tau\\sim\\pi_{\\theta}}[R]=E_{\\tau}[\\sum_{t=0}^{\\tau-1}G_t \\nabla_{\\theta}log\\pi_{\\theta}(a_t|s_t)]$\n",
        "\n",
        "* 其中 $G_t=\\sum_{t^{\\prime}=t}^{\\tau-1}r_{t^{\\prime}}$ 就是在 $t^{\\prime}$ 时刻的总 reward "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44KMiAkf1n2V"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smxXXbzj2E3Z"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(Agent, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 128)\n",
        "        self.fc2 = nn.Linear(128, 2)\n",
        "        self.dropout = nn.Dropout(p=0.6)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # self.current_policy = \n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        policy = self.softmax(x)\n",
        "        return policy"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVTygoBcBJ9a"
      },
      "source": [
        "# Emulator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iytH1XFOBMuj"
      },
      "source": [
        "import gym\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class Emulator:\n",
        "    def __init__(self, name, gamma=0.9):\n",
        "        self.env = gym.make(name)\n",
        "        self.gamma = gamma\n",
        "        self.current_state = None\n",
        "        self.reward_seq = []\n",
        "\n",
        "    def step(self, agent):\n",
        "        action_probs = agent(torch.from_numpy(self.current_state).float().unsqueeze(0))\n",
        "        action = Categorical(action_probs).sample().item()\n",
        "        self.current_state, reward, done, _ = self.env.step(action)\n",
        "        self.reward_seq.append(reward)\n",
        "\n",
        "\n",
        "        \n",
        "        return self.current_state\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_state = self.env.reset()\n",
        "        self.reward_seq = []\n",
        "\n",
        "    def get_Gt(self):\n",
        "        for i in self.reward_seq[::-1]:\n",
        "            "
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFMac4sv2Hjk",
        "outputId": "17486735-0d42-420d-adee-fc5d974e8c6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "agent = Agent()\n",
        "emulator = Emulator('CartPole-v1')\n",
        "\n",
        "\n",
        "emulator.reset()\n",
        "emulator.step(agent)\n",
        "emulator.step(agent)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.03397693, -0.02535876,  0.0074242 ,  0.03120025])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttv9FnRw2R6w",
        "outputId": "7ef8b1c8-3204-4a95-c8e2-aa9464bab441",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "emulator.reward_list"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek52uQQp2TCY",
        "outputId": "4f1dd5a8-0e5e-45c7-8ee8-e17380856742",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env.step(0)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-0.08764318, -1.71546854,  0.27073459,  2.93805168]), 0.0, True, {})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUkxxYct2VNR"
      },
      "source": [
        "env = gym.make('CartPole-v1')"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKJbwC94Ce1B",
        "outputId": "9fdad1b6-3d58-4552-a341-995b556fb433",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env.reset()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.02754705, -0.00197631, -0.00116885, -0.02404558])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMjZuyXgCgYY",
        "outputId": "f08bc8ac-be3b-42b9-962c-a8e1d99666b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env.step(0)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-0.02758658, -0.19708147, -0.00164976,  0.26826833]), 1.0, False, {})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P-M_ImSChy5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}