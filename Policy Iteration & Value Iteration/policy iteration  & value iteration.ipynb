{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算寻路小游戏各状态 Value Function\n",
    "\n",
    "1. 红色各自初始位置\n",
    "2. 黑色点获得奖励-10\n",
    "3. 黄色点获得奖励100\n",
    "4. 每个状态的向4各方向的转移概率相同，假设不会撞墙\n",
    "\n",
    "![title](WechatIMG1201.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-21.07, -20.83, -23.97, -15.3 ],\n",
       "       [-20.83, -29.33, -50.  , -11.77],\n",
       "       [-23.97, -50.  ,  89.96,  24.89],\n",
       "       [-15.3 , -11.77,  24.89,  18.91]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 状态转移矩阵\n",
    "S = np.array([\n",
    "       [0, 1/2, 0., 0., 0., 1/2, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], #1\n",
    "       [1/3, 0, 1/3, 0., 0., 1/3, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], #2\n",
    "       [0., 1/3, 0, 1/3, 0., 0., 1/3, 0., 0., 0., 0., 0., 0., 0., 0., 0.], #3\n",
    "       [0., 0., 1/2, 0, 0., 0., 0., 1/2, 0., 0., 0., 0., 0., 0., 0., 0.], #4\n",
    "       [1/3, 0., 0., 0., 0, 1/3, 0., 0., 1/3, 0., 0., 0., 0., 0., 0., 0.], #5\n",
    "       [0., 1/4, 0., 0., 1/4, 0., 1/4, 0., 0., 1/4, 0., 0., 0., 0., 0., 0.], #6\n",
    "       [0., 0., 0., 0., 0., 0., 1, 0., 0., 0., 0., 0., 0., 0., 0., 0.], ##7 \n",
    "       [0., 0., 0., 1/3, 0., 0., 1/3, 0, 0., 0., 0., 1/3, 0., 0., 0., 0.], #8\n",
    "       [0., 0., 0., 0., 1/3, 0., 0., 0., 0, 1/3, 0., 0., 1/3, 0., 0., 0.], #9\n",
    "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1, 0., 0., 0., 0., 0., 0.], ##10\n",
    "       [0., 0., 0., 0., 0., 0., 1/4, 0., 0., 1/4, 0., 1/4, 0., 0., 1/4, 0.], #11\n",
    "       [0., 0., 0., 0., 0., 0., 0., 1/3, 0., 0., 1/3, 0, 0., 0., 0., 1/3], #12\n",
    "       [0., 0., 0., 0., 0., 0., 0., 0., 1/2, 0., 0., 0., 0, 1/2, 0., 0.], #13\n",
    "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1/3, 0., 0., 1/3, 0, 1/3, 0.], #14\n",
    "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1/3, 0., 0., 1/3, 0, 1/3], #15\n",
    "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1/2, 0., 0., 1/2, 0], #16\n",
    "])\n",
    "\n",
    "\n",
    "# 状态奖励\n",
    "R = np.array([-1, -1, -1, -1, -1, -1, -10, -1, -1, -10, 100, -1, -1, -1, -1, -1])\n",
    "\n",
    "# 衰减因子\n",
    "gamma = 0.8\n",
    "\n",
    "# 阈值更新\n",
    "threshold = 0.0001\n",
    "\n",
    "# 迭代计算\n",
    "V_init = np.ones(16)\n",
    "V = np.zeros(16)\n",
    "\n",
    "while abs((V_init - V).sum()) > threshold:\n",
    "    V = V_init\n",
    "    V_init = R + gamma * np.dot(V, S.T)\n",
    "    \n",
    "np.round(V, 2).reshape(4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略迭代与价值迭代\n",
    "\n",
    "1. 红色为起始点\n",
    "2. 黑色为惩罚点，获得-10奖励\n",
    "3. 黄色为目标点，获得100的奖励\n",
    "4. 撞墙的状态奖励设置为-5，撞墙状态所有action回到自身\n",
    "5. 每个状态均有上下左右4种action\n",
    "    1. 边缘状态遇到撞墙action则回到自己状态\n",
    "    2. 黄色，黑色点只有所有action均回到自身\n",
    "    3. 其他点对应4个action均到达相应指向的状态 $p(s^{\\prime}|a)=1$\n",
    "\n",
    "\n",
    "![title](WX20201126-003224.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# envirement\n",
    "S = np.arange(0, 17)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每个 state 的状态价值初始化\n",
    "V = np.zeros(17)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -5.,   0.,   0.,   0.,   0.,   0.,   0., -10.,   0.,   0., -10.,\n",
       "       100.,   0.,   0.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每个 state 的 reward\n",
    "R = np.zeros(17)\n",
    "R[0] = -5\n",
    "R[[7, 10]] = -10\n",
    "R[11] = 100\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每个 state 的 policy，横 -> 上下左右，纵 -> state，值 -> acion 对应的概率，均等初始化\n",
    "P = np.ones(shape=(17, 4)) / 4\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个 state 的 action(上下左右) 对应的下一个 state 的\n",
    "A = np.array([\n",
    "       [0, 0, 0, 0], # 0\n",
    "       [0, 5, 1, 2],\n",
    "       [0, 6, 1, 3],\n",
    "       [0, 7, 2, 4],\n",
    "       [0, 8, 3, 4],\n",
    "       [1, 9, 0, 6],\n",
    "       [2, 10,5, 7],\n",
    "       [7, 7, 7, 7], # 7\n",
    "       [4, 12,7, 0],\n",
    "       [5, 13,0, 10],\n",
    "       [10,10,10,10],# 10\n",
    "       [11,11,11,11],# 11\n",
    "       [8, 16,11,0],\n",
    "       [9, 0, 0,14],\n",
    "       [10,0 ,13,15],\n",
    "       [11,0 ,14,16],\n",
    "       [12,0 ,15, 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mdp():\n",
    "    S = np.arange(0, 17)\n",
    "    V = np.zeros(17)\n",
    "    R = np.zeros(17)\n",
    "    R[0] = -1000\n",
    "    R[[7, 10]] = -500\n",
    "    R[11] = 999\n",
    "    P = np.ones(shape=(17, 4)) / 4\n",
    "    A = np.array([\n",
    "       [0, 0, 0, 0], # 0\n",
    "       [0, 5, 0, 2], # 1\n",
    "       [0, 6, 1, 3], # 2\n",
    "       [0, 7, 2, 4], # 3\n",
    "       [0, 8, 3, 0], # 4\n",
    "       [1, 9, 0, 6], # 5\n",
    "       [2, 10,5, 7], # 6\n",
    "       [7, 7, 7, 7], # 7\n",
    "       [4, 12,7, 0], # 8\n",
    "       [5, 13,0, 10],# 9\n",
    "       [10,10,10,10],# 10\n",
    "       [11,11,11,11],# 11\n",
    "       [8, 16,11,0], # 12\n",
    "       [9, 0, 0,14], # 13\n",
    "       [10,0 ,13,15],# 14\n",
    "       [11,0 ,14,16],# 15\n",
    "       [12,0 ,15, 0] # 16\n",
    "    ])\n",
    "    return S, V, R, P, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 策略评估与策略提升"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, value, reward, policy, action, gamma):\n",
    "    new_value = value.copy()\n",
    "    for s in env:\n",
    "        Gt = reward[s]\n",
    "        for a, p in zip(action[s], policy[s]):\n",
    "            Gt += value[a] * p * gamma\n",
    "        new_value[s] = Gt\n",
    "    return new_value\n",
    "\n",
    "def policy_improvement(env, value, reward, policy, action):\n",
    "    new_policy = policy.copy()\n",
    "    for s in env:\n",
    "        action_value = []\n",
    "        for a, p in zip(action[s], policy[s]):\n",
    "            action_value.append(value[a] * p)\n",
    "        \n",
    "        max_value = np.array(action_value).max()\n",
    "        max_value_idx = []\n",
    "        \n",
    "        for i, v in enumerate(action_value):\n",
    "            if v == max_value:\n",
    "                max_value_idx.append(i)\n",
    "            else:\n",
    "                action_value[i] = 0\n",
    "        action_value_array = np.array(action_value)\n",
    "        action_value_array[max_value_idx] = 1 / len(max_value_idx)\n",
    "        new_policy[s] = action_value_array\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 策略迭代\n",
    "\n",
    "1. $v(s)$ 到收敛\n",
    "2. $\\pi(s)$ 迭代一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def poliy_iteration(env, value, reward, policy, action, gamma, threshold):\n",
    "    policy_last = np.zeros(shape=policy.shape)\n",
    "    \n",
    "    while not (policy_last == policy).all():\n",
    "        policy_last = policy.copy()\n",
    "        \n",
    "        # 策略评估到 value function 收敛\n",
    "        value_last = np.ones(shape=value.shape)\n",
    "        while  np.abs(value_last - value).mean() > threshold:\n",
    "            value_last = value.copy()\n",
    "            value = policy_evaluation(env, value, reward, policy, action, gamma)\n",
    "        \n",
    "        # 更新策略最价值最大的 action\n",
    "        policy = policy_improvement(env, value, reward, policy, action)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.5 , 0.  , 0.5 ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.5 , 0.  , 0.5 , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.5 , 0.  , 0.5 , 0.  ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S, V, R, P, A = get_mdp()\n",
    "policy_iteration_optimal_policy = poliy_iteration(S, V, R, P, A, 0.8, 0.00001)[1:]\n",
    "policy_iteration_optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 价值迭代\n",
    "\n",
    "1. 当前最大的 $\\pi(s)$ 迭代一次求得 $v(s)$\n",
    "2. 最大的 $v(s)$ 与当前 $v(s)$ 相差计算\n",
    "3. 直到阈值到收敛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_max_policy_value_function(env, value, reward, policy, action, gamma):\n",
    "    new_value = value.copy()\n",
    "    for s in env:\n",
    "        max_Gt = -float('inf')\n",
    "        for a, p in zip(action[s], policy[s]):\n",
    "            Gt_a = p * (reward[a] + gamma * value[a])\n",
    "            if Gt_a > max_Gt:\n",
    "                max_Gt = Gt_a\n",
    "        new_value[s] = max_Gt\n",
    "    return new_value\n",
    "\n",
    "def value_iteration(env, value, reward, policy, action, gamma, threshold):\n",
    "    value_temp = np.ones(shape=value.shape)\n",
    "    \n",
    "    while np.abs(value_temp - value).mean() > threshold:\n",
    "        value_temp = value.copy()\n",
    "        value = get_current_max_policy_value_function(env, value_temp, reward, policy, action, gamma)\n",
    "        \n",
    "    optimal_policy = policy_improvement(env, value, reward, policy, action)\n",
    "    return optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.5 , 0.  , 0.5 ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.5 , 0.  , 0.5 , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.5 , 0.  , 0.5 , 0.  ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S, V, R, P, A = get_mdp()\n",
    "value_iteration_optimal_policy = value_iteration(S, V, R, P, A, 0.8, 0.00001)[1:]\n",
    "value_iteration_optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## policy 优化结果\n",
    "\n",
    "![title](结果.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "两种迭代方式结果是否一致 True\n"
     ]
    }
   ],
   "source": [
    "print('两种迭代方式结果是否一致', (value_iteration_optimal_policy == policy_iteration_optimal_policy).all())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
